{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_document() function generates 'document_data' tuple that contains the data associated with each documen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D1': 'Researchers have successfully applied large language models (LLMs) such as ChatGPT to reranking in an information retrieval context, but to date, such work has mostly been built on proprietary models hidden behind opaque API endpoints. This approach yields experimental results that are not reproducible and non-deterministic, threatening the veracity of outcomes that build on such shaky foundations. To address this significant shortcoming, we present RankVicuna, the first fully open-source LLM capable of performing high-quality listwise reranking in a zero-shot setting. Experimental results on the TREC 2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness comparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter model, although our effectiveness remains slightly behind reranking with GPT-4. We hope our work provides the foundation for future research on reranking with modern LLMs.', 'D2': 'With the advent of transformer-based architectures, the contextual representation of text data has leveraged the query and the document to be represented in low-dimensional dense vector space. These vectors are learned embeddings of fixed sizes, resulting in deeper text understanding. In this study, we designed a pipeline for effectively retrieving documents from a large search space by combining the deeper text understanding capabilities of the transformer-based BERT model and a phrase embedding-based query expansion model. To learn the contextual representations, we fine-tuned a deep semantic matching model by separately encoding the document and the query. The encoder model is based on the Sentence BERT (SBERT) architecture, which separately generates dense vector representations of documents and queries. The study has also addressed the maximum token length limitation of transformer-based models through the summarization of lengthy documents. In addition, to improve the clarity and completeness of short queries and reduce the semantic gap, a phrase embedding-based query expansion model is employed. The documents and their dense vectors are indexed using the Elasticsearch engine, and matched them with query vectors for retrieving query-specific documents. Finally, the BERT-based cross-encoder model is used to re-rank the relevant records for each query. It performs full self-attention over the inputs, and yields richer text interactions to produce the final results. To assess performance, experiments are conducted on two well-known datasets, TREC-CDS-2014 and OHSUMED. A comparative analysis is carried out, which clearly demonstrates that the proposed framework produced competitive retrieval results.', 'D3': 'In scientific research, the ability to effectively retrieve relevant documents based on complex, multifaceted queries is critical. Existing evaluation datasets for this task are limited, primarily due to the high cost and effort required to annotate resources that effectively represent complex queries. To address this, we propose a novel task, Scientific DOcument Retrieval using Multi-level Aspect-based quEries (DORIS-MAE), which is designed to handle the complex nature of user queries in scientific research. We developed a benchmark dataset within the field of computer science, consisting of 100 human-authored complex query cases. For each complex query, we assembled a collection of 100 relevant documents and produced annotated relevance scores for ranking them. Recognizing the significant labor of expert annotation, we also introduce Anno-GPT, a scalable framework for validating the performance of Large Language Models (LLMs) on expert-level dataset annotation tasks. LLM annotation of the DORIS-MAE dataset resulted in a 500x reduction in cost, without compromising quality. Furthermore, due to the multi-tiered structure of these complex queries, the DORIS-MAE dataset can be extended to over 4,000 sub-query test cases without requiring additional annotation. We evaluated 17 recent retrieval methods on DORIS-MAE, observing notable performance drops compared to traditional datasets. This highlights the need for better approaches to handle complex, multifaceted queries in scientific research.', 'D4': 'Large Language Models (LLMs) demonstrate impressive effectiveness in zero-shot document ranking tasks. Pointwise, Pairwise, and Listwise prompting approaches have been proposed for LLM-based zero-shot ranking. Our study begins by thoroughly evaluating these existing approaches within a consistent experimental framework, considering factors like model size, token consumption, latency, among others. This first-of-its-kind comparative evaluation of these approaches allows us to identify the trade-offs between effectiveness and efficiency inherent in each approach. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. To further enhance the efficiency of LLM-based zero-shot ranking, we propose a novel Setwise prompting approach. Our approach reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, significantly improving the efficiency of LLM-based zero-shot ranking. We test our method using the TREC DL datasets and the BEIR zero-shot document ranking benchmark. The empirical results indicate that our approach considerably reduces computational costs while also retaining high zero-shot ranking effectiveness. ', 'D5': 'Dense embedding-based retrieval is now the industry standard for semantic search and ranking problems, like obtaining relevant web documents for a given query. Such techniques use a two-stage process: (a) contrastive learning to train a dual encoder to embed both the query and documents and (b) approximate nearest neighbor search (ANNS) for finding similar documents for a given query. These two stages are disjoint; the learned embeddings might be ill-suited for the ANNS method and vice-versa, leading to suboptimal performance. In this work, we propose End-to-end Hierarchical Indexing -- EHI -- that jointly learns both the embeddings and the ANNS structure to optimize retrieval performance. EHI uses a standard dual encoder model for embedding queries and documents while learning an inverted file index (IVF) style tree structure for efficient ANNS. To ensure stable and efficient learning of discrete tree-based ANNS structure, EHI introduces the notion of dense path embedding that captures the position of a query/document in the tree. We demonstrate the effectiveness of EHI on several benchmarks, including de-facto industry standard MS MARCO (Dev set and TREC DL19) datasets. For example, with the same compute budget, EHI outperforms state-of-the-art (SOTA) in by 0.6% (MRR@10) on MS MARCO dev set and by 4.2% (nDCG@10) on TREC DL19 benchmarks. ', 'D6': 'Query expansion (QE) is commonly used to improve the performance of traditional information retrieval (IR) models. With the adoption of deep learning in IR research, neural QE models have emerged in recent years. Many of these models focus on learning embeddings by leveraging query-document relevance. These embedding models allow computing semantic similarities between queries and documents to generate expansion terms. However, existing models often ignore query-document interactions. This research aims to address that gap by proposing a QE model using a conditional variational autoencoder. It first maps a query-document pair into a latent space based on their interaction, then estimates an expansion model from that latent space. The proposed model is trained on relevance feedback data and generates expansions using pseudo-relevance feedback at test time. The proposed model is evaluated on three standard TREC collections for document ranking: AP and Robust 04 and GOV02, and the MS MARCO dataset for passage ranking. Results show the model outperforms state-of-the-art traditional and neural QE models. It also demonstrates higher additivity with neural matching than baselines.'}\n"
     ]
    }
   ],
   "source": [
    "def load_documents():\n",
    "    \n",
    "    directory_path = \"./\"\n",
    "    file_list = [\"/d1.txt\", \"/d2.txt\", \"/d3.txt\", \"/d4.txt\",\"/d5.txt\",\"/d6.txt\"]\n",
    "    document_data = {}\n",
    "\n",
    "    for idx, filename in enumerate(file_list, 1):       \n",
    "        file = open(directory_path + filename)\n",
    "        file_content = file.read()\n",
    "        file.close()\n",
    "        document_data[f'D{idx}'] = file_content\n",
    "    return document_data\n",
    "\n",
    "print(load_documents())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "description file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(document_data,tokenise,normalise):\n",
    "    processed_data = {}\n",
    "    for doc_id, content in document_data.items():\n",
    "        # Tokenization\n",
    "        if (tokenise==\"Split\"):\n",
    "            tokens=content.split()\n",
    "        else:\n",
    "            ExpReg = nltk.RegexpTokenizer('(?:[A-Z]\\.)+|\\d+(?:\\.\\d+)?DA?|\\w+|\\.{3}')\n",
    "            tokens = ExpReg.tokenize(content)\n",
    "        # Remove stopwords\n",
    "        motsvides = nltk.corpus.stopwords.words('english')\n",
    "        tokens_without_stopw = [token for token in tokens if token.lower() not in motsvides]\n",
    "\n",
    "        # Stemming using the Lancaster stemmer\n",
    "        if (normalise==\"Lancaster\"):\n",
    "            Lancaster = nltk.LancasterStemmer()\n",
    "            termes_normalization = [Lancaster.stem(terme) for terme in tokens_without_stopw]\n",
    "        else:\n",
    "            Porter = nltk.PorterStemmer()\n",
    "            termes_normalization = [Porter.stem(terme) for terme in tokens_without_stopw]\n",
    "\n",
    "        processed_data[doc_id] = termes_normalization\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_frequencies(processed_data):\n",
    "    term_frequencies = {}\n",
    "    for doc_id, terms in processed_data.items():\n",
    "        term_frequencies[doc_id] = defaultdict(int)\n",
    "        for term in terms:\n",
    "            term_frequencies[doc_id][term] += 1\n",
    "\n",
    "    return term_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_weights(term_frequencies, processed_data, num_documents):\n",
    "    term_weights = {}\n",
    "    for doc_id, terms in processed_data.items():\n",
    "        max_term_freq = max(term_frequencies[doc_id].values())\n",
    "        term_weights[doc_id] = {}\n",
    "        for term, freq in term_frequencies[doc_id].items():\n",
    "            term_weights[doc_id][term] = (freq / max_term_freq) * math.log(num_documents / (1 + sum(1 for d in processed_data if term in processed_data[d])))\n",
    "\n",
    "    return term_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inverted file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index_file(processed_data):\n",
    "    inverted_index = defaultdict(list)\n",
    "    term_frequencies = calculate_term_frequencies(processed_data)\n",
    "\n",
    "    for doc_id, terms in processed_data.items():\n",
    "        for term in terms:\n",
    "            inverted_index[term].append(doc_id)\n",
    "\n",
    "    return inverted_index, term_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndocument_data = load_documents()  # Load the documents\\nprocessed_data = preprocess_data(document_data, 'Split', 'Lancaster')\\nnum_documents = len(document_data)\\nterm_frequencies = calculate_term_frequencies(processed_data)\\nterm_weights = calculate_term_weights(term_frequencies, processed_data, num_documents)\\ndescriptorfile=descriptorfile(processed_data,term_weights)\\nprint(descriptorfile)\\n\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def descriptorfile(processed_data, term_weights):\n",
    "    all_descriptors = []\n",
    "    term_frequencies = calculate_term_frequencies(processed_data)\n",
    "    for doc_id, terms in processed_data.items():\n",
    "        for term in terms:\n",
    "            frequency = term_weights[doc_id][term]\n",
    "            all_descriptors.append(f\"{doc_id}: {term} - Frequency: {term_frequencies[doc_id][term]} - Weight: {frequency}\")\n",
    "\n",
    "    return all_descriptors\n",
    "\"\"\"\n",
    "document_data = load_documents()  # Load the documents\n",
    "processed_data = preprocess_data(document_data, 'Split', 'Lancaster')\n",
    "num_documents = len(document_data)\n",
    "term_frequencies = calculate_term_frequencies(processed_data)\n",
    "term_weights = calculate_term_weights(term_frequencies, processed_data, num_documents)\n",
    "descriptorfile=descriptorfile(processed_data,term_weights)\n",
    "print(descriptorfile)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'document_data=load_documents()\\nprocessed_data = preprocess_data(document_data)\\nnum_documents = len(document_data)\\nterm_frequencies = calculate_term_frequencies(processed_data)\\nterm_weights = calculate_term_weights(term_frequencies, processed_data, num_documents)'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''document_data=load_documents()\n",
    "processed_data = preprocess_data(document_data)\n",
    "num_documents = len(document_data)\n",
    "term_frequencies = calculate_term_frequencies(processed_data)\n",
    "term_weights = calculate_term_weights(term_frequencies, processed_data, num_documents)'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the files and save them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'document_data=load_documents()\\n# Create a descriptor file for all documents\\ndescriptor_filename = \"all_descriptors.txt\"\\nall_descriptors = descriptorfile(processed_data, term_weights)\\nwith open(descriptor_filename, \"w\") as file:\\n    for descriptor in all_descriptors:\\n        file.write(f\"{descriptor}\\n\")\\nprint(f\"Descriptor file created for all documents: {descriptor_filename}\")\\n#inverted \\ninverted_index, term_frequencies = create_inverted_index(processed_data)\\n\\nwith open(\"inverted_index.txt\", \"w\") as text_file:\\n    for term, doc_list in inverted_index.items():\\n        text_file.write(f\"{term}:\\n\")\\n        for doc_id in doc_list:\\n            term_freq = term_frequencies.get(doc_id, {}).get(term, 0)\\n            term_weight = term_weights.get(doc_id, {}).get(term, 0.0)\\n            text_file.write(f\"  {doc_id}: Frequency: {term_freq} - Weight: {term_weight}\\n\")\\n\\n\\nprint(\"Inverted index with term weights saved to \\'inverted_index.txt.\\'\")'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''document_data=load_documents()\n",
    "# Create a descriptor file for all documents\n",
    "descriptor_filename = \"all_descriptors.txt\"\n",
    "all_descriptors = descriptorfile(processed_data, term_weights)\n",
    "with open(descriptor_filename, \"w\") as file:\n",
    "    for descriptor in all_descriptors:\n",
    "        file.write(f\"{descriptor}\\n\")\n",
    "print(f\"Descriptor file created for all documents: {descriptor_filename}\")\n",
    "#inverted \n",
    "inverted_index, term_frequencies = create_inverted_index(processed_data)\n",
    "\n",
    "with open(\"inverted_index.txt\", \"w\") as text_file:\n",
    "    for term, doc_list in inverted_index.items():\n",
    "        text_file.write(f\"{term}:\\n\")\n",
    "        for doc_id in doc_list:\n",
    "            term_freq = term_frequencies.get(doc_id, {}).get(term, 0)\n",
    "            term_weight = term_weights.get(doc_id, {}).get(term, 0.0)\n",
    "            text_file.write(f\"  {doc_id}: Frequency: {term_freq} - Weight: {term_weight}\\n\")\n",
    "\n",
    "\n",
    "print(\"Inverted index with term weights saved to 'inverted_index.txt.'\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def descriptor(processed_data, term_weights, term_frequencies):\n",
    "    all_descriptors = []\n",
    "    for doc_id, terms in processed_data.items():\n",
    "        for term in terms:\n",
    "            frequency = term_weights[doc_id][term]\n",
    "            doc_term_tuple = (doc_id, term, term_frequencies[doc_id][term], frequency)\n",
    "            all_descriptors.append(doc_term_tuple)\n",
    "\n",
    "    return all_descriptors\n",
    "\n",
    "\n",
    "def inverted_index(processed_data, term_frequencies, term_weights):\n",
    "    inverted_index = defaultdict(list)\n",
    "\n",
    "    for doc_id, terms in processed_data.items():\n",
    "        for term in terms:\n",
    "            term_freq = term_frequencies.get(doc_id, {}).get(term, 0)\n",
    "            term_weight = term_weights.get(doc_id, {}).get(term, 0.0)\n",
    "            term_doc_tuple = (term, doc_id, term_freq, term_weight)\n",
    "            inverted_index[term].append(term_doc_tuple)\n",
    "\n",
    "    return inverted_index\n",
    "#desc=descriptor(processed_data, term_weights, term_frequencies)\n",
    "#inv=inverted_index(processed_data, term_frequencies, term_weights)\n",
    "def get_terms_by_doc_id(all_descriptors, target_doc_id):\n",
    "    terms_with_doc_id = [item for item in all_descriptors if item[0] == target_doc_id]\n",
    "    return terms_with_doc_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_terms_by_term_inverted(inverted_index, target_term):\n",
    "    terms_with_target_term = []\n",
    "    term_doc_tuples = inverted_index.get(target_term, [])\n",
    "    terms_with_target_term.extend(term_doc_tuples)\n",
    "    return terms_with_target_term\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inverted_index = inverted_index(processed_data, term_frequencies, term_weights)\n",
    "target_doc_id = \"D1\"  # Replace with the doc_id you want to filter by\n",
    "terms_with_doc_id = get_terms_by_doc_id_inverted(inverted_index, target_doc_id)\n",
    "for term_tuple in terms_with_doc_id:\n",
    "    print(term_tuple)\n",
    "\n",
    "\n",
    "\n",
    "    # Example usage:\n",
    "'''\n",
    "all_descriptors = descriptor(processed_data, term_weights, term_frequencies)\n",
    "target_doc_id = \"D1\"  # Replace with the doc_id you want to filter by\n",
    "terms_with_doc_id = get_terms_by_doc_id(all_descriptors, target_doc_id)\n",
    "\n",
    "for term_tuple in terms_with_doc_id:\n",
    "    print(term_tuple)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define a function to handle the retrieval and display\n",
    "def retrieve_and_display():\n",
    "    selected_docs = doc_var.get()\n",
    "    tokenization_method = tokenization_method_var.get()\n",
    "    stemming_method = stemming_method_var.get()\n",
    "    output_type = output_type_var.get()\n",
    "    \n",
    "    # Split the selected_docs string to get a list of selected document IDs\n",
    "    selected_doc_ids = selected_docs.split(\", \")\n",
    "\n",
    "    # Load the relevant data (inverted index or descriptors)\n",
    "    if output_type == \"DOCS per TERM\":\n",
    "        relevant_data = inverted_index(processed_data, term_frequencies, term_weights)\n",
    "        terms_with_doc_id_inverted = get_terms_by_term_inverted(relevant_data, selected_docs)\n",
    "        \n",
    "        # Display the selected document data\n",
    "        result_text.delete(1.0, tk.END)  # Clear previous results\n",
    "        if terms_with_doc_id_inverted:\n",
    "            for term_tuple in terms_with_doc_id_inverted:\n",
    "                result_text.insert(tk.END, f\"{term_tuple}\\n\")\n",
    "        else:\n",
    "            result_text.insert(tk.END, \"No document for this term.\\n\")\n",
    "        \n",
    "    elif output_type == \"TERMS per DOC\":\n",
    "        relevant_data = descriptor(processed_data, term_weights, term_frequencies)\n",
    "        terms_with_doc_id = get_terms_by_doc_id(relevant_data, selected_docs)\n",
    "        \n",
    "        # Display the selected document data\n",
    "        result_text.delete(1.0, tk.END)  # Clear previous results\n",
    "        if terms_with_doc_id:\n",
    "            for data in terms_with_doc_id:\n",
    "                result_text.insert(tk.END, f\"{data}\\n\")\n",
    "        else:\n",
    "            result_text.insert(tk.END, \"No term for this document.\\n\")\n",
    "\n",
    "# Rest of the code remains the same\n",
    "\n",
    "\n",
    "# Create the main application window\n",
    "app = tk.Tk()\n",
    "app.title(\"Document Retrieval\")\n",
    "\n",
    "# Create labels and entry fields for user input\n",
    "doc_label = ttk.Label(app, text=\"Query:\")\n",
    "doc_var = tk.StringVar()\n",
    "doc_entry = ttk.Entry(app, textvariable=doc_var,width=40)\n",
    "doc_label.grid(row=0, column=0, padx=5, pady=5)\n",
    "doc_entry.grid(row=0, column=1, padx=5, pady=5)  # Place the text field\n",
    "\n",
    "tokenization_label = ttk.Label(app, text=\"Select Tokenization Method:\")\n",
    "tokenization_method_var = tk.StringVar()\n",
    "tokenization_method_var.set(\"Split\")\n",
    "tokenization_method_combobox = ttk.Combobox(app, textvariable=tokenization_method_var, values=[\"Split\", \"Tokenize\"])\n",
    "\n",
    "stemming_label = ttk.Label(app, text=\"Select Stemming Method:\")\n",
    "stemming_method_var = tk.StringVar()\n",
    "stemming_method_var.set(\"Lancaster\")\n",
    "stemming_method_combobox = ttk.Combobox(app, textvariable=stemming_method_var, values=[\"Lancaster\", \"Porter\"])\n",
    "\n",
    "output_label = ttk.Label(app, text=\"Index:\")\n",
    "output_type_var = tk.StringVar()\n",
    "output_type_var.set(\"DOCS per TERM\")\n",
    "output_type_combobox = ttk.Combobox(app, textvariable=output_type_var, values=[\"DOCS per TERM\", \"TERMS per DOC\"])\n",
    "\n",
    "retrieve_button = ttk.Button(app, text=\"Search\", command=retrieve_and_display)\n",
    "\n",
    "# Create a text widget for displaying the results\n",
    "result_text = tk.Text(app, width=90, height=30)\n",
    "\n",
    "# Arrange the widgets on the GUI\n",
    "tokenization_label.grid(row=1, column=0, padx=5, pady=5)\n",
    "tokenization_method_combobox.grid(row=1, column=1, padx=5, pady=5)\n",
    "stemming_label.grid(row=2, column=0, padx=5, pady=5)\n",
    "stemming_method_combobox.grid(row=2, column=1, padx=5, pady=5)\n",
    "output_label.grid(row=3, column=0, padx=5, pady=5)\n",
    "output_type_combobox.grid(row=3, column=1, padx=5, pady=5)\n",
    "retrieve_button.grid(row=4, column=0, columnspan=2, padx=5, pady=10)\n",
    "result_text.grid(row=5, column=0, columnspan=2, padx=5, pady=5)\n",
    "\n",
    "# Start the GUI application\n",
    "app.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
